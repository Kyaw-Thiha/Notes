# Maths behind GMM
#ml/classic-models/mixture-model/gaussian/maths  

![Maths Behind GMM|500](https://miro.medium.com/v2/resize:fit:2000/1*iRTc264XIPghpllPlh7JUg.gif)

`Problem Formulation`

Let $\{ y_{i} \}^N_{i=1}$, $y_{i} \in R^d$ be the `dataset`.

Let $K$ be the no. of `Gaussian component densities`, 
each with $\mu_{j} \in R^d$ and $C_{j} \in R^{d \times d}$ such that
- $\mu_{1:K}$ are $K$ `mean vectors`
- $C_{1:K}$ are $K$ `covariance matrices`

Let $m_{1:K}$ be the `mixing probabilities` $\begin{cases}m_{j} \in [0,1] \\[6pt] \sum^K_{j=1} m_{j}  = 1\end{cases}$
where $m_{j}$ is fraction of data generated by $j^{th}$ Gaussian component.

Let $\theta = \{ m_{1:K}, \mu_{1:K}, C_{1:K} \}$ be the parameters to learn.

---
`Component Likelihood`
Since each component likelihood is modelled as [[Gaussian Distribution]], we get
$$
p(y \ | \ l=j, \theta) = G(y; \ \mu_{j}, C_{j})
$$
where
- $\mu_{j}$ is the `mean` of $j^{th}$ gaussian component
- $C_{j}$ is the `covariance matrix` of $j^{th}$ gaussian component

`Posterior (Responsibilities)`
We can use [[Bayes Rule in ML]] to derive
$$
\begin{align}
\gamma_{i,j}
&= p(l=j \ | \ y_{i}, \theta) \\[6pt]
&= \frac{p(l=j \ | \ \theta) \ p(y_{i} \ | \ l=j, \theta)} 
{p(y_{i} | \theta)} \\[6pt]
&= \frac{m_{j} \ p(y_{i} \ | \ l=j, \theta)} 
{p(y_{i} | \theta)} \\[6pt]
\end{align}
$$
---
`GMM Data Likelihood`
$$
\begin{align}
p(y | \theta)
&= \sum^k_{j=1} p(y, \ l=j \ | \ \theta) 
\ , \quad \text{by marginalization over latent variable } l \\[6pt]

&= \sum^k_{j=1} p(l=j \ | \ \theta) \  
p(y \ | \ l=j,\theta)  
\ , \quad \text{by prior } \times \text{ conditional likelihood}\\[6pt]

&= \sum^k_{j=1} m_{j} \ G(y; \ \mu_{j}, C_{j}) \\[6pt]

&= \sum^k_{j=1} m_{j}  
\left( \frac{1}{(2\pi^d) |C_{j}|} \right)^{1/2} 
\exp\left( -\frac{1}{2} (y-\mu_{j})^T C_{j}^{-1} (y - \mu_{j}) \right)
\end{align}
$$

Hence, we get
$$
p(y | \theta)
= \sum^k_{j=1} m_{j}  
\left( \frac{1}{(2\pi^d) |C_{j}|} \right)^{1/2} 
\exp\left( -\frac{1}{2} (y-\mu_{j})^T C_{j}^{-1} (y - \mu_{j}) \right)
$$

We can draw category $l \in \{ 1, \dots, K \}$ from `categorical distribution` with probability $m_{j}$
Given $l$, draw an observation $y$ from component $l$
$$
y \sim N(\mu_{l}, C_{l})
$$

---
`Objective Function`

Goal of [[Gaussian Mixture Model]] is find the $\theta$ that maximizes the `data likelihood`.
Thus, we can minimize the [[Log Likelihood|Negative Log Likelihood]] of data.

We can define `objective function` as
$$
L(\theta) = - \log \ p(y_{1:N} \ | \ \theta)
$$

---
`MLE Estimate`

Assume data points $y_{1:N}$ are $I.I.D$.
Then,
$$
\begin{align}
L(\theta)
&= -\log p(y_{1:N} \ | \ \theta) \\[6pt]
&= -\log \prod^N_{i=1} p(y_{i} \ | \ \theta) \\[6pt]
&= -\sum^N_{i=1} \log \sum^K_{j=1}  
m_{j} \ p(y_{i} \ | \ l=j, \theta) 
\end{align}
$$
where $\theta = \{ m_{1:K}, \mu_{1:K}, C_{1:K} \}$

We also need to define the `constraints` as
$$
1. \begin{cases}
m_{j} \in [0, 1] \\[6pt]
\sum^K_{j=1} m_{j} = 1
\end{cases}
\ \quad  \
2. \ C_{j} \text{ is symmetric positive definite}
$$

---
## See Also
- [[Gaussian Mixture Model]]
- [[Mixture Model]]
# TD(Î»)
#rl/td-lambda

`TD(Î»)` combines the strengths of [[Monte Carlo]] and [[Temporal Difference]] by using *multiple-step lookahead updates* weighted by a **decay parameter** `Î»`.

![[TD Lambda.png]]

## Intuition
Instead of updating the value function using only the immediate next state (as in one-step TD), `TD(Î»)` averages over all *n-step returns*, blending short-term TD and long-term Monte Carlo information.

- When `Î» = 0` â†’ becomes **TD(0)** (pure 1-step TD).
- When `Î» = 1` â†’ becomes **Monte Carlo** (full-episode return).
- For `0 < Î» < 1`, the update is a **weighted mix** of n-step TD targets:
  
  $$
  G_t^{(Î»)} = (1 - Î») \sum_{n=1}^{âˆž} Î»^{n-1} G_t^{(n)}
  $$



## Update Rule
The general TD(Î») update for state values:

$$
V(S_t) \leftarrow V(S_t) + \alpha [G_t^{(Î»)} - V(S_t)]
$$

In the **online (backward view)** using *eligibility traces*:

$$
E_t(S) = Î³Î»E_{t-1}(S) + ðŸ™(S = S_t)
$$

$$
V(S) \leftarrow V(S) + Î± [R_{t+1} + Î³V(S_{t+1}) - V(S_t)] \, E_t(S)
$$

Here:
- $E_t(S)$ tracks how recently and frequently a state was visited.
- $Î»$ controls how long past states remain â€œeligibleâ€ for updates.

---

# See Also
- [[Temporal Difference]]
- [[Monte Carlo]]

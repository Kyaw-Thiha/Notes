# Interview Practice Set A — Balanced Technical + Behavioural

## 1. Background & Motivation
- Tell me about yourself and your background in machine learning.
- Why are you interested in joining NRC?
- What attracted you specifically to model evaluation and benchmarking?

## 2. Python / PyTorch
- Walk me through a Python project where you automated or optimized something.
- How do you debug Python scripts that give inconsistent outputs?
- Describe your experience writing PyTorch training or evaluation loops.
- How would you implement a custom loss function for an object tracking task?

## 3. Object Detection & Tracking
- Explain the difference between object detection and object tracking.
- What challenges arise when tracking objects in videos?
- What metrics do you use to evaluate detection models? What about tracking models?

## 4. Benchmarking & Evaluation Pipeline
- How would you design a benchmarking pipeline to compare several detection models?
- What can go wrong in an evaluation pipeline?
- How would you ensure reproducibility across experiments?
- How would you handle mismatched annotation formats from different datasets?

## 5. Problem Solving
- If one model is fast but inaccurate and another is slower but precise, which would you choose for real-time applications?
- A model performs well on synthetic data but poorly on real videos — how do you investigate?
- Your evaluation metrics fluctuate a lot across runs; what are the possible causes?

## 6. Teamwork / Communication
- Describe a time when you collaborated on a complex project.
- How would you document your benchmarking pipeline for future researchers?
- How do you explain complex ML concepts to non-technical stakeholders?

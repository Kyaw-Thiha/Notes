# Practical HRL Stack — Reward Design & Credit Assignment (Survey Table)
Target stack: **MAXQ** (hierarchical value decomposition) + **Option-Critic** (learned skills) + **HIRO** (off‑policy, goal‑conditioned) + **Intrinsic Motivation** (DIAYN/adInfoHRL/HIDIO) for scaling from **5×5** → **25×25** hex maps with varied terrain and multi‑unit control.

| Technique                                            | What it does (key idea / formula)                                                                                                                                                                                    | Why it’s relevant to **your hybrid**                                                                                                                                     | Where it plugs in (level / module)                                                                          | Key papers (links)                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| ---------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **Potential‑Based Reward Shaping (PBRS)**            | Adds shaping term $F(s,a,s')=\gamma\,\Phi(s')-\Phi(s)$ that preserves optimal policies; often equivalent to smart Q‑init.                                                                                            | Gives dense hints (e.g., supply connectivity, VP proximity, encirclement progress) without biasing the final policy. Works alongside MAXQ pseudo‑rewards and OC options. | Tactical & operational subtasks (potentials for supply/zone control); anneal weight as training progresses. | Ng et al. 1999 — Policy invariance under shaping ([pdf](https://people.eecs.berkeley.edu/~russell/papers/icml99-shaping.pdf)); Wiewiora 2003 — PBRS ≈ Q‑init ([abs](https://arxiv.org/abs/1106.5267)); PBRS‑MAXQ‑0 (PBRS in HRL) ([preprint](https://www.researchgate.net/publication/336812578_Potential_Based_Reward_Shaping_for_Hierarchical_Reinforcement_Learning))                                                                                                       |
| **MAXQ pseudo‑rewards**                              | Decompose task/value into subtasks with terminal pseudo‑rewards for local success.                                                                                                                                   | Clear credit within tactical/operational goals (e.g., “secure bridgehead”, “cut rail link”), while high‑level stays sparse.                                              | Subtask boundaries (terminate with pseudo‑reward only); combine with PBRS inside the subtask.               | Dietterich 2000 — MAXQ ([JAIR](https://jair.org/index.php/jair/article/view/10266), [arXiv](https://arxiv.org/pdf/cs/9905014))                                                                                                                                                                                                                                                                                                                                                 |
| **Option‑Critic (OC)** + **Deliberation Cost**       | Learn intra‑option policies, terminations, and policy‑over‑options. Deliberation cost penalizes frequent switching → longer, interpretable options.                                                                  | Stabilizes learned **skills** (hold/assault/raid), reduces thrashing, matches military “commitment” to a maneuver.                                                       | Skill layer (OC); termination regularizer (deliberation cost).                                              | Bacon et al. 2016 — OC ([arXiv](https://arxiv.org/abs/1609.05140), [pdf](https://scispace.com/pdf/the-option-critic-architecture-10sjeox2hn.pdf)); Harb et al. 2018 — Deliberation cost ([pdf](https://cdn.aaai.org/ojs/11831/11831-13-15359-1-2-20201228.pdf)); Natural OC ([pdf](https://cdn.aaai.org/ojs/4452/4452-13-7491-1-10-20190706.pdf)); DAC options ([pdf](https://papers.neurips.cc/paper/8475-dac-the-double-actor-critic-architecture-for-learning-options.pdf)) |
| **HIRO (off‑policy GC)** + **HER**                   | HIRO: high‑level proposes latent goals; low‑level trained off‑policy with correction; HER relabels failed goals with achieved ones for sample efficiency.                                                            | Efficient scaling from 5×5 to 25×25: subgoal geometry (bridges, chokepoints, depots) trains fast; pairs naturally with OC workers.                                       | High‑level goal setter (HIRO) + low‑level goal‑conditioned controller; add HER in replay.                   | Nachum et al. 2018 — HIRO ([NeurIPS pdf](https://papers.neurips.cc/paper/7591-data-efficient-hierarchical-reinforcement-learning.pdf)); Andrychowicz et al. 2017 — HER ([abs](https://arxiv.org/abs/1707.01495), [pdf](https://papers.neurips.cc/paper/7090-hindsight-experience-replay.pdf))                                                                                                                                                                                  |
| **Intrinsic Motivation** (DIAYN / adInfoHRL / HIDIO) | Discover diverse skills via information‑theoretic objectives (maximize $I(\text{skill};\text{states})$); deterministic options with advantage‑weighted MI (adInfoHRL); HIDIO learns task‑agnostic intrinsic options. | Pretrain a **skill zoo** (flanking, screening, probing) to explore sparse terrains; plug into OC/HIRO to cut exploration time.                                           | Pretraining & auxiliary rewards; option discovery layer.                                                    | DIAYN 2018 ([abs](https://arxiv.org/abs/1802.06070)); adInfoHRL 2019 ([pdf](https://arxiv.org/pdf/1901.01365)); HIDIO 2021 ([abs](https://arxiv.org/abs/2101.06521), [slides](https://iclr.cc/media/Slides/iclr/2021/virtual%2806-16-00%29-06-16-00UTC-2805-hierarchical_re.pdf))                                                                                                                                                                                              |
| **Reward Machines (RMs)** / **LTL shaping**          | Encode multi‑phase missions as automata; give dense, PBRS‑compatible progress rewards. LTL‑based shaping supports average‑reward settings.                                                                           | Formalizes complex operations: *recon → cut supply → seize → hold*. Clean ablations & interpretability.                                                                  | Mission spec layer (environment wrapper or high‑level supervisor).                                          | Toro Icarte et al. 2018 — RMs ([pdf](https://proceedings.mlr.press/v80/icarte18a/icarte18a.pdf)); Jiang et al. 2021 — LTL reward shaping ([pdf](https://cdn.aaai.org/ojs/16975/16975-13-20469-1-2-20210518.pdf)); Camacho et al. 2019 — LTL & rewards ([pdf](https://www.ijcai.org/proceedings/2019/0840.pdf))                                                                                                                                                                 |
| **Reward Redistribution (RUDDER)**                   | Redistribute long‑delayed returns to decisive steps using contribution analysis; makes expected future rewards $\approx 0$.                                                                                          | Credits pivotal actions (e.g., cutting a single rail hex) in long campaigns; accelerates learning without inventing dense heuristics.                                    | Critic/return preprocessing; offline/auxiliary model that writes redistributed rewards to replay.           | Arjona‑Medina et al. 2018/2019 ([abs](https://arxiv.org/abs/1806.07857), [NeurIPS](https://proceedings.neurips.cc/paper/2019/hash/16105fb9cc614fc29e1bda00dab60d41-Abstract.html))                                                                                                                                                                                                                                                                                             |
| **Multi‑agent credit** (Difference Rewards / COMA)   | Difference reward: $D_i=G(z)-G(z^{-i})$ isolates agent $i$’s contribution. **COMA**: centralized critic, counterfactual advantage for decentralized actors.                                                          | Many‑unit control under CTDE: reduces free‑riding, stabilizes coordination; integrates with OC/HIRO training.                                                            | Training‑time only (centralized learner); decentralized execution at test.                                  | Foerster et al. 2018 — COMA ([pdf](https://www.cs.ox.ac.uk/people/shimon.whiteson/pubs/foersteraaai18.pdf)); Devlin et al. 2014 — Potential‑based difference rewards ([pdf](https://citeseerx.ist.psu.edu/document?doi=a5ca908bac6dcc907a700d4e975dfc7e51a3fd89&repid=rep1&type=pdf)); Castellini et al. 2020 — Dr.Reinforce ([abs](https://arxiv.org/abs/2012.11258))                                                                                                         |
| **RTS/Strategy practice** (ELF Mini‑RTS, μRTS)       | Prior work uses shaped proxies (economy, unit kills, structure damage) + curricula and action masking to make RTS learning feasible.                                                                                 | Borrow shaping terms & ablation templates; benchmark transfer from small to larger maps before moving to your hex wargame.                                               | Experimental design & baselines.                                                                            | ELF (NeurIPS’17) ([pdf](https://papers.neurips.cc/paper/6859-elf-an-extensive-lightweight-and-flexible-research-platform-for-real-time-strategy-games.pdf)); Gym‑μRTS 2021 ([abs](https://arxiv.org/abs/2105.13807)); DRL μRTS winner 2024 ([pdf](https://arxiv.org/pdf/2402.08112))                                                                                                                                                                                           |

---

## Minimal wiring suggestions (how to actually use them)
- **PBRS backbone**: $\Phi=$ supply connectivity + VP/zone control + force cohesion; add $F=\gamma\Phi(s')-\Phi(s)$ to *all* levels, then **anneal**.  
- **MAXQ pseudo‑rewards** only at subtask termination; inside each subtask use PBRS + HER for goal reaching.  
- **OC** with a **deliberation cost** at tactical layer to enforce temporal commitment.  
- **HIRO+HER**: high‑level proposes position/subgraph goals (bridges, rail nodes); HER relabels achieved goals.  
- **Intrinsic** (DIAYN/adInfoHRL/HIDIO): pretrain a diverse skill set → initialize OC options.  
- **RUDDER**: offline pass to reweight rewards in long scenarios (campaigns).  
- **CTDE credit**: train with **COMA/difference rewards**, execute decentralized.  


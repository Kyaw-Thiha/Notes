# Agnostic PAC Learning
#ml/statistical-learning/agnostic-pac-learning
`Agnostic PAC Learning` is [[PAC Learning]] that removes the assumption of `realizability`.

> Essentially, it admits that no perfect `hypothesis` exists.
> Either because hypothesis class is too simple, or data is too noisy.

---
## Giving up on Realizability
Recall that realizability requires
$$
\exists h^* \in \mathcal{H} \text{ s.t }
p_{x \sim \mathcal{D}}[h^*(x) = f(x)] \ = 1
$$
where
- $\mathcal{D}: \mathcal{X} \to [0, 1]$ is the data distribution
- $f: \mathcal{X} \to \mathcal{Y}$ is the true target function

`Modelling Noise`
Now, let's replace $\mathcal{D}$ and $f(x)$ with a new joint distribution $\mathcal{D}$ over $\mathcal{X} \times \mathcal{Y}$.

This means that the input samples and their labels are both generated randomly.
This allows us to model noise in the labels.

Breaking $\mathcal{D}$ down into two components, we get
1. $\mathcal{D_{\mathcal{X}}}$: The `marginal distribution` over input domain $\mathcal{X}$
2. $\mathcal{D}_{y \ | \ x}$: The `conditional distribution` over labels for points in the domain $\mathcal{Y}$

---
## Empirical Risk
We now define true error with respect to the new distribution $\mathcal{D}$.
$$
\begin{align}
L_{\mathcal{D}}  
&\triangleq p_{(x, y) \sim \mathcal{D}} [h(x) \neq y]
\\[6pt]

&\triangleq D(\{ x, y \}: h(x) \neq y)
\\[6pt]
\end{align}
$$

However, the [[Empirical Risk Minimization#Empirical Risk|Empirical Risk]] remains the same: 
$$
L_{S}(h)
\triangleq \frac{1}{m}
|\{ i \in [m]: h(x_{i}) \neq y_{i} \}|
$$

Hence, the [[Empirical Risk Minimization]] becomes
> Find a hypothesis $h: \mathcal{X} \to \mathcal{Y}$ that $(\text{probably approximately})$ minimizes the true risk $L_{\mathcal{D}}(h)$.

---
## Agnostic PAC Learning Definition

A `hypothesis class` $\mathcal{H}$ is `agnostic PAC learnable` if there exists 
- a function $m_{\mathcal{H}}: ] 0,1 [^2 \to \mathbb{N}$ 
- and a learning algorithm $A(S)$

with the following property

> - For every $\epsilon, \delta \in ] 0,1 [$ and for every distribution $\mathcal{D}$ over $\mathcal{X} \times \mathcal{Y}$, 
> - when running the algorithm on $m \geq m_{\mathcal{H}}(\epsilon, \delta)$ $i.i.d$ examples generated by $\mathcal{D}$
> - $A(S)$ returns a hypothesis $h$ such that
$$
L_{\mathcal{D}}(h) 
\leq \min_{h' \in \mathcal{H}} L_{\mathcal{D}}(h') 
+ \epsilon
$$
> with a probability of at least $1-\delta$ $(\text{confidence parameter})$.

---


# AdaBoost Error Bound
#proof

> For the hypothesis generated by [[AdaBoost]], the [[Empirical Risk]](training error) is bounded by

$$
\mathbb{1}( \ h(x) \neq y \ )
\leq e^{-y \cdot h(x)}
$$
---
### Proof
Assume that $y, h(x) \in \{ -1, 1 \}$ (`binary classification`)
We are going to proof by cases.

**Case-1**: $h(x)=y$
Then,
- $\mathbb{1}(h(x) \neq y) = 0$ 
- $e^{-y \cdot h(x)} = e^{-y\cdot y} = e^{-1} \approx 0.368 > 0$

Hence $0 < 0.368 \implies \mathbb{1}( \ h(x) \neq y \ ) \leq e^{-y \cdot h(x)}$, as wanted.

**Case-2**: $h(x) \neq y$
Then,
- $\mathbb{1}(h(x) \neq y) = 1$ 
- $e^{-y \cdot h(x)} = e^{-y \cdot -y} = e^{y^2} = e^{1} \approx 2.718 > 1$

Hence $1 < 2.718 \implies \mathbb{1}( \ h(x) \neq y \ ) \leq e^{-y \cdot h(x)}$, as wanted.

---
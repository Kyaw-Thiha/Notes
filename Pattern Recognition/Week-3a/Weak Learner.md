# Weak Learner

To quantify a `weak learner`, we can relax our constraints on accuracy.
This will reduce the computational difficulty of learning.

---
`Definition`
A learning algorithm $A$ is a $\gamma\text{-weak learner}$ for a class $\mathcal{H}$ 
if there exists a function $m_{\mathcal{H}}(0,1) \to \mathbb{N}$ such that
- for every $\delta \in \ ] \ 0,1 \ [$ ,
- for every `distribution` $\mathcal{D}$ over $\mathcal{X}$, and
- for every `labelling function` $f: \mathcal{X} \to \{ +1 \}$

if the `realizability assumption` holds with respect to $\mathcal{H}, \mathcal{D}, f$,
then when running the learning algorithm on $m \geq m_{\mathcal{H}}(\delta)$ $\text{i.i.d}$ examples generated by $\mathcal{D}$ and labelled by $\mathcal{f}$, the algorithm returns a hypothesis $h$ such that
$$
L_{\mathcal{D},f}(h)
\leq \frac{1}{2} - \gamma
$$
with a probability of $1-\delta$.

---
`Weak Learnability`
A hypothesis class $\mathcal{H}$ is $\gamma \text{-weak learnable}$ if there exists a $\gamma \text{-weak learner}$ for that class.

---
`Weak Learner Notes`
This is similar to [[PAC Learning]], except that $\epsilon$ is replaced with $\frac{1}{2} - \gamma$ where $\gamma \in \ ]0, 0.5[$.

Essentially this means that probability of mis-classification has to be less than $50\%$.
In other words, it has to be better than random guessing.

---
## See Also
- [[PAC Learning]]
- [[AdaBoost]]
- [[Maths behind AdaBoost]]